# Experiment brainstorm

Unless you’re running a [mini Chaos Event](../../complementary-approaches/running-a-mini-chaos-event.md), schedule a planning session at least two weeks ahead of Chaos Day. This allows engineering time for the selected experiments to be designed, implemented and tested ahead of Chaos Day itself.

Gather the participants together and facilitate a brainstorming session, with the above [context](experiment-brainstorm.md#context) in mind. Have the team sketch out the system architecture and steady-state behaviour (e.g. typical traffic and expected error levels). A boxes and arrows sketch is fine: you just need a high enough level that’s easy to visualise and reason about, but low enough to identify components that might fail. If the number of participants is small and time allows it, there is added value in having each person draw their own sketch, then compare them one by one, starting with that done by the person least familiar with the system. Even this activity alone can improve the resilience of the system because it corrects any misconceptions within the team and gives each member deeper architectural and behavioural understanding, which they can draw on during the next production incident.

Before brainstorming failures, decide on what the scope of the experiments should be, considering:&#x20;

1. Which parts of the system do you not have control over? Failures will be out of scope if their remediations are outside your team’s control (e.g. an internal failure within a cloud provider, such as an AWS network outage).
2. Which components are off-limits for experimentation? If another team has critical work in progress that requires component X to be highly available on Chaos Day, it might be best to preserve team harmony by declaring component X to be off-limits.
3. What is the desired blast radius? Failures in one component or whole service will often impact connected components and downstream services. Decide if there should be a constraint on how far failures are allowed to ripple.
4. Is there a desired focus area or theme to orient experiments around? Review the [experiment themes](experiment-brainstorm.md#context) and decide if Chaos Day will be a smorgasbord or a set menu of experiment types.

With a common architecture diagram and scope agreed upon and visible to everyone, the next step is to brainstorm the experiments. Ask each participant to spend the next 5-10 minutes brainstorming 3-8 failures they’d like to learn more about, writing each failure on a sticky note (or virtual equivalent) and placing it on the architecture diagram near the target component/connection. For each failure, they should consider the failure event (what, where, how), expected impact and expected response.

At the end of the brainstorm, affinity group similar ideas and discuss those of particular interest (use [dot-voting](https://en.wikipedia.org/wiki/Dot-voting) if necessary). Firm facilitation is required here, as these discussions often get lost in detail. Keep the group focused on reaching the point where they can agree on the top 4-8 experiments that optimise for learning within the agreed scope and theme. The prioritisation process should be a discussion where the following attributes are identified and compared for each experiment:

* _Impact_ - if the failure really happened how significant would the business and/or reputational damage be? How would this change over the course of the failure, until normal service is resumed?
* _Likelihood_ - how likely is the failure? Has it happened before? What conditions would have to be present for it to occur and how likely are those conditions? Don’t spend time on failures whose conditions represent a much wider impact, such as national or planet-scale outages, but don’t also rule out failures that seem unlikely. At one client, we used a highly available, cloud database service, fully managed by a leading public cloud provider. We had absolute confidence it would never fail, until one day a regular infrastructure-as-code deployment deleted the database and its backups (ironically due to cloud configuration changes designed to increase resilience). Question all assumptions!
* _Expected response_ (MTTR) - how do you expect the system and team around it to respond when the failure occurs? Will a [circuit breaker](https://martinfowler.com/bliki/CircuitBreaker.html) kick in and requests be later retried? Will alerts fire? Will the team quickly notice and know what to do?
* _Mental model_ - how well understood is the failure and the impacted architectural parts? Complex systems that are hard to reason about are also hard to support, especially when something goes wrong.
* _Organisational benefits_, such as cross-team relationships - resilient systems are built and operated by teams. Failures that span multiple teams require a cross-team response. How well do the impacted teams know each other? Do they have established points of contact and communication protocols?

Having short-listed 4-8 experiments, agree on an owner for each experiment and set the expectation that, before the Chaos Day, the owners will have fleshed out their experiments to the point they are confident in executing them. The next section describes a template we’ve found useful in guiding experiment design.
